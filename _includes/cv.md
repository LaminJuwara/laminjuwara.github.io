My name is Lamin Juwara. I am second year student in the biostatistic program at McGill  working on privacy preserving data analysis under the supervisions of  [Dr Paramita Saha-Chaudhuri](https://sites.google.com/site/paramitasaharesearch/) and [Dr Alexandra M Schmidt](http://alex-schmidt.research.mcgill.ca/). Prior to mcgill, I studied at [AIMS](https://www.aims.ac.za/en/home)- Stellenbosch University (MSc Mathematics) under the supervision of [Dr Wilfred Ndifon](https://scholar.google.com/citations?user=T7leliwAAAAJ&hl=en) and the Kwame Nkrumah University of Science and Tech, Ghana (BSc Mathematics).




## <i class="fa fa-chevron-right"></i> News
<table class="table table-hover">
<tr>
  <td class='col-md-3'>April 2018</td>
  <td>I will  present a poster ("Microaggregation as a Privacy-Preserving Analytical Tool for Analysis of Confidential Distributed Data") at the ISPE Mid-Year Meeting. <a href='https://www.pharmacoepi.org/meetings/mid-year-2018/'> Learning Awareness Models</a>.</td>
</tr>
<tr>
  <td class='col-md-3'>Feb 2018</td>
  <td> I presented a poster "Virtual Pooling as a Privacy-preserving Analysis Tool to Estimate Covariate Hazard Ratio (HR) of Cox Proportional Hazard Model" which won the best poster award at the EBOSS Research Day.</td>
</tr>
<tr>
  <td class='col-md-3'>May 2017</td>
  <td>I am interning at <a href='http://www.sacema.org/'> The South African Centre for Epidemiological Modelling and Analysis.</a> in Stellenbosch this summer with <a href='http://www.sacema.org/people/staff'>Prof Alex Welta</a>.</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td class="col-md-3">Aug 2016 - June 2018</td>
    <td>
        <strong>MSc. in Biostatistics</strong>
        <br>
      McGill University
    </td>
  </tr>
  <tr>
    <td class="col-md-3">Aug 2015 - May 2016</td>
    <td>
        <strong>MSc in Mathematics Sciences</strong>
        <br>
      AIMS South Africa, Stellenbosch University
    </td>
  </tr>
  <tr>
    <td class="col-md-3">Aug 2011 - May 2015</td>
    <td>
        <strong>B.Sc. in Mathematics (Hons)</strong>
        <br>
      Kwame Nkrumah University of Science and Technology
    </td>
  </tr>

</table>


## <i class="fa fa-chevron-right"></i> Research Experience
<table class="table table-hover">
<tr>
  <td class='col-md-3'>Apr 2016 - Present</td>
  <td>
    <strong>Carnegie Mellon University</strong>, Zico Kolter <br>
    Machine learning and optimization
  </td>
</tr>
<tr>
  <td class='col-md-3'>June 2018 - Sept 2018</td>
  <td>
    <strong>Intel Labs</strong>, Vladlen Koltun <br>
    Machine learning
  </td>
</tr>
<tr>
  <td class='col-md-3'>May 2017 - Oct 2017</td>
  <td>
    <strong>Google DeepMind</strong>, Nando de Freitas <br>
    Machine and reinforcement learning
  </td>
</tr>
<tr>
  <td class='col-md-3'>Aug 2014 - Apr 2016</td>
  <td>
    <strong>Carnegie Mellon University</strong>, Mahadev Satyanarayanan <br>
    Applied machine learning and mobile systems
  </td>
</tr>
<tr>
  <td class='col-md-3'>May 2012 - May 2014</td>
  <td>
    <strong>Virginia Tech</strong>, Jules White <br>
    Mobile systems, cyber-physical systems, and security
  </td>
</tr>
<tr>
  <td class='col-md-3'>Jan 2013 - May 2014</td>
  <td>
    <strong>Virginia Tech</strong>, Layne Watson <br>
    Scientific computing, global/stochastic optimization, and bioinformatics
  </td>
</tr>
<tr>
  <td class='col-md-3'>Nov 2012 - Mar 2014</td>
  <td>
    <strong>Virginia Tech</strong>, Binoy Ravindran <br>
    Heterogeneous compilers
  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Selected Publications <a href="https://github.com/bamos/cv/blob/master/publications/selected.bib"><i class="fa fa-code-fork" aria-hidden="true"></i></a>

<a href="https://scholar.google.com/citations?user=d8gdZR4AAAAJ" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a>

<table class="table table-hover">

<tr>
<td class="col-md-3"><a href='https://openreview.net/forum?id=r1HhRfWRZ' target='_blank'><img src="images/publications/amos2018learning.png"/></a> </td>
<td>
    <strong>Learning Awareness Models</strong><br>
    <strong>B. Amos</strong>, L. Dinh, S. Cabi, T. Roth&ouml;rl, S. Colmenarejo, A. Muldal, T. Erez, Y. Tassa, N. de Freitas, and M. Denil<br>
    ICLR 2018<br>
    
    [1] 
[<a href='javascript: none'
    onclick='$("#abs_amos2018learning").toggle()'>abs</a>] [<a href='https://openreview.net/forum?id=r1HhRfWRZ' target='_blank'>pdf</a>] <br>
    
<div id="abs_amos2018learning" style="text-align: justify; display: none" markdown="1">
We consider the setting of an agent with a fixed body interacting with an
unknown and uncertain external world. We show that models
trained to predict proprioceptive information about the
agent's body come to represent objects in the external world.
In spite of being trained with only internally available
signals, these dynamic body models come to represent external
objects through the necessity of predicting their effects on
the agent's own body. That is, the model learns holistic
persistent representations of objects in the world, even
though the only training signals are body signals. Our
dynamics model is able to successfully predict distributions
over 132 sensor readings over 100 steps into the future and we
demonstrate that even when the body is no longer in contact
with an object, the latent variables of the dynamics model
continue to represent its shape. We show that active data
collection by maximizing the entropy of predictions about the
body-touch sensors, proprioception and vestibular
information-leads to learning of dynamic models that show
superior performance when used for control. We also collect
data from a real robotic hand and show that the same models
can be used to answer questions about properties of objects in
the real world. Videos with qualitative results of our models
are available <a href="https://goo.gl/mZuqAV">here</a>.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='http://arxiv.org/abs/1703.04529' target='_blank'><img src="images/publications/donti2017task.png"/></a> </td>
<td>
    <strong>Task-based End-to-end Model Learning</strong><br>
    P. Donti, <strong>B. Amos</strong>, and J. Kolter<br>
    NIPS 2017<br>
    
    [2] 
[<a href='javascript: none'
    onclick='$("#abs_donti2017task").toggle()'>abs</a>] [<a href='http://arxiv.org/abs/1703.04529' target='_blank'>pdf</a>]  [<a href='https://github.com/locuslab/e2e-model-learning' target='_blank'>code</a>] <br>
    
<div id="abs_donti2017task" style="text-align: justify; display: none" markdown="1">
As machine learning techniques have become more ubiquitous, it has
become common to see machine learning prediction algorithms operating
within some larger process. However, the criteria by which we train
machine learning algorithms often differ from the ultimate criteria on
which we evaluate them. This paper proposes an end-to-end approach for
learning probabilistic machine learning models within the context of
stochastic programming, in a manner that directly captures the
ultimate task-based objective for which they will be used. We then
present two experimental evaluations of the proposed approach, one as
applied to a generic inventory stock problem and the second to a
real-world electrical grid scheduling task. In both cases, we show
that the proposed approach can outperform both a traditional modeling
approach and a purely black-box policy optimization approach.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='http://arxiv.org/abs/1703.00443' target='_blank'><img src="images/publications/amos2017optnet.png"/></a> </td>
<td>
    <strong>OptNet: Differentiable Optimization as a Layer in Neural Networks</strong><br>
    <strong>B. Amos</strong> and J. Kolter<br>
    ICML 2017<br>
    
    [3] 
[<a href='javascript: none'
    onclick='$("#abs_amos2017optnet").toggle()'>abs</a>] [<a href='http://arxiv.org/abs/1703.00443' target='_blank'>pdf</a>]  [<a href='https://github.com/locuslab/optnet' target='_blank'>code</a>] <br>
    
<div id="abs_amos2017optnet" style="text-align: justify; display: none" markdown="1">
This paper presents OptNet, a network architecture that integrates
optimization problems (here, specifically in the form of quadratic programs)
as individual layers in larger end-to-end trainable deep networks.
These layers encode constraints and complex dependencies
between the hidden states that traditional convolutional and
fully-connected layers often cannot capture.
In this paper, we explore the foundations for such an architecture:
we show how techniques from sensitivity analysis, bilevel
optimization, and implicit differentiation can be used to
exactly differentiate through these layers and with respect
to layer parameters;
we develop a highly efficient solver for these layers that exploits fast
GPU-based batch solves within a primal-dual interior point method, and which
provides backpropagation gradients with virtually no additional cost on top of
the solve;
and we highlight the application of these approaches in several problems.
In one notable example, we show that the method is
capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game;
this highlights the ability of our architecture to learn hard
constraints better than other neural architectures.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='http://arxiv.org/abs/1609.07152' target='_blank'><img src="images/publications/amos2017input.png"/></a> </td>
<td>
    <strong>Input Convex Neural Networks</strong><br>
    <strong>B. Amos</strong>, L. Xu, and J. Kolter<br>
    ICML 2017<br>
    
    [4] 
[<a href='javascript: none'
    onclick='$("#abs_amos2017input").toggle()'>abs</a>] [<a href='http://arxiv.org/abs/1609.07152' target='_blank'>pdf</a>]  [<a href='https://github.com/locuslab/icnn' target='_blank'>code</a>] <br>
    
<div id="abs_amos2017input" style="text-align: justify; display: none" markdown="1">
This paper presents the input convex neural network
architecture. These are scalar-valued (potentially deep) neural
networks with constraints on the network parameters such that the
output of the network is a convex function of (some of) the inputs.
The networks allow for efficient inference via optimization over some
inputs to the network given others, and can be applied to settings
including structured prediction, data imputation, reinforcement
learning, and others. In this paper we lay the basic groundwork for
these models, proposing methods for inference, optimization and
learning, and analyze their representational power. We show that many
existing neural network architectures can be made input-convex with
a minor modification, and develop specialized optimization
algorithms tailored to this setting. Finally, we highlight the
performance of the methods on multi-label prediction, image
completion, and reinforcement learning problems, where we show
improvement over the existing state of the art in many cases.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='http://www.cs.cmu.edu/~hzhao1/papers/ICML2016/BL-SPN-main.pdf' target='_blank'><img src="images/publications/zhao2016collapsed.png"/></a> </td>
<td>
    <strong>Collapsed Variational Inference for Sum-Product Networks</strong><br>
    H. Zhao, T. Adel, G. Gordon, and <strong>B. Amos</strong><br>
    ICML 2016<br>
    
    [5] 
[<a href='javascript: none'
    onclick='$("#abs_zhao2016collapsed").toggle()'>abs</a>] [<a href='http://www.cs.cmu.edu/~hzhao1/papers/ICML2016/BL-SPN-main.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_zhao2016collapsed" style="text-align: justify; display: none" markdown="1">
Sum-Product Networks (SPNs) are probabilistic inference machines that admit
exact inference in linear time in the size of the network. Existing
parameter learning approaches for SPNs are largely based on the maximum
likelihood principle and hence are subject to overfitting compared to
more Bayesian approaches. Exact Bayesian posterior inference for SPNs is
computationally intractable. Both standard variational inference and
posterior sampling for SPNs are computationally infeasible even for
networks of moderate size due to the large number of local latent
variables per instance. In this work, we propose a novel deterministic
collapsed variational inference algorithm for SPNs that is
computationally efficient, easy to implement and at the same time allows
us to incorporate prior information into the optimization formulation.
Extensive experiments show a significant improvement in accuracy compared
with a maximum likelihood based approach.
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='http://reports-archive.adm.cs.cmu.edu/anon/anon/2016/CMU-CS-16-118.pdf' target='_blank'><img src="images/publications/amos2016openface.png"/></a> </td>
<td>
    <strong>OpenFace: A general-purpose face recognition library with mobile applications</strong><br>
    <strong>B. Amos</strong>, B. Ludwiczuk, and M. Satyanarayanan<br>
    CMU 2016<br>
    
    [6] 
[<a href='javascript: none'
    onclick='$("#abs_amos2016openface").toggle()'>abs</a>] [<a href='http://reports-archive.adm.cs.cmu.edu/anon/anon/2016/CMU-CS-16-118.pdf' target='_blank'>pdf</a>]  [<a href='https://cmusatyalab.github.io/openface' target='_blank'>code</a>] <br>
    
<div id="abs_amos2016openface" style="text-align: justify; display: none" markdown="1">
Cameras are becoming ubiquitous in the Internet of Things (IoT) and
can use face recognition technology to improve context. There is a
large accuracy gap between today's publicly available face recognition
systems and the state-of-the-art private face recognition
systems. This paper presents our OpenFace face recognition library
that bridges this accuracy gap. We show that OpenFace provides
near-human accuracy on the LFW benchmark and present a new
classification benchmark for mobile scenarios. This paper is intended
for non-experts interested in using OpenFace and provides a light
introduction to the deep neural network techniques we use.

We released OpenFace in October 2015 as an open source library under
the Apache 2.0 license. It is available at:
<http://cmusatyalab.github.io/openface/>
</div>

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://vtechworks.lib.vt.edu/bitstream/handle/10919/49672/qnTOMS14.pdf' target='_blank'><img src="images/publications/amos2014QNSTOP.png"/></a> </td>
<td>
    <strong>QNSTOP-QuasiNewton Algorithm for Stochastic Optimization</strong><br>
    <strong>B. Amos</strong>, D. Easterling, L. Watson, W. Thacker, B. Castle, and M. Trosset<br>
    VT 2014<br>
    
    [7] 
[<a href='javascript: none'
    onclick='$("#abs_amos2014QNSTOP").toggle()'>abs</a>] [<a href='https://vtechworks.lib.vt.edu/bitstream/handle/10919/49672/qnTOMS14.pdf' target='_blank'>pdf</a>] <br>
    
<div id="abs_amos2014QNSTOP" style="text-align: justify; display: none" markdown="1">
QNSTOP consists of serial and parallel (OpenMP) Fortran 2003 codes for the
quasi-Newton stochastic optimization method of Castle and Trosset. For
stochastic problems, convergence theory exists for the particular
algorithmic choices and parameter values used in QNSTOP. Both the parallel
driver subroutine, which offers several parallel decomposition strategies, and the serial driver subroutine can be used for stochastic optimization or
deterministic global optimization, based on an input switch. QNSTOP is
particularly effective for “noisy” deterministic problems, using only
objective function values. Some performance data for computational systems
biology problems is given.
</div>

</td>
</tr>


</table>


## <i class="fa fa-chevron-right"></i> Teaching Experience
<table class="table table-hover">
<tr>
  <td class='col-md-1'>S2017</td>
  <td><strong>Graduate AI</strong> (CMU 15-780), TA</td>
</tr>
<tr>
  <td class='col-md-1'>S2016</td>
  <td><strong>Distributed Systems</strong> (CMU 15-440/640), TA</td>
</tr>
<tr>
  <td class='col-md-1'>S2013</td>
  <td><strong>Software Design and Data Structures</strong> (VT CS 2114), TA</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Industry Experience
<table class="table table-hover">
<tr>
  <td class='col-md-3'>May 2014 - Aug 2014</td>
  <td><strong>Adobe Research</strong>, Data Scientist Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>Dec 2013 - Jan 2014</td>
  <td><strong>Snowplow Analytics</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2013 - Aug 2013</td>
  <td><strong>Qualcomm</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2012 - Aug 2012</td>
  <td><strong>Phoenix Integration</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>Jan 2011 - Aug 2011</td>
  <td><strong>Sunapsys</strong>, Network Administrator Intern</td>
</tr>
<tr>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> CMU Graduate Coursework
+ Statistical Machine Learning (10-702, Au), L. Wasserman, S2017
+ Deep Reinforcement Learning (10-703, Au), R. Salakhutdinov and A. Fragkiadaki, S2017
+ Intermediate Statistics (10-705, Au), L. Wasserman, F2016
+ Topics in Deep Learning (10-807), R. Salakhutdinov, F2016
+ Convex Optimization (10-725), R. J. Tibshirani, F2015
+ Algorithms in the Real World (15-853), G. Blelloch and A. Gupta, F2015
+ Semantics of Programming Languages (15-812), A. Platzer, S2015
+ Optimizing Compilers for Modern Architecture (15-745), T. Mowry, S2015
+ Advanced Operating and Distributed Systems (15-712), D. Andersen, F2014
+ Mobile and Pervasive Computing (15-812), M. Satyanarayanan and D. Siewiorek, F2014


## <i class="fa fa-chevron-right"></i> Honors & Awards
<table class="table table-hover">
<tr>
  <td class='col-md-2'>2016-2018</td>
  <td>
    MCF Scholar, McGill U
    <!--  -->
  </td>
</tr>
<tr>
  <td class='col-md-2'>2016</td>
  <td>
    The Martin Rees Scholarship
    <!--  -->
  </td>
</tr>
<tr>
  <td class='col-md-2'>2015</td>
  <td>
    African Institute for Mathematical Sciences MSc scholarship
    <!--  -->
  </td>
</tr>
<tr>
  <td class='col-md-2'>2015</td>
  <td>
    Best graduating student, BSc Mathematics; Rank: 1/156
    <!--  -->
  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Skills
<table class="table table-hover">
<tr>
  <td class='col-md-2'>Languages</td>
  <td markdown="1">
Python, R, MatLab
  </td>
</tr>
<tr>
  <td class='col-md-2'>Frameworks</td>
  <td markdown="1">
NumPy, Pandas, SciPy
  </td>
</tr>
<tr>
  <td class='col-md-2'>Systems</td>
  <td markdown="1">
Linux, Windows
  </td>
</tr>
</table>




### Posters 

<table class="table table-hover">

<tr>
<td>
    <strong>Virtual Pooling as a Privacy-preserving Analysis Tool to Estimate
Covariate Hazard Ratio (HR) of Cox Proportional Model</strong><br>
    <strong>Lamin Juwara</strong> , Alexandra M Schmidt, PhD and Paramita Saha-Chaudhuri, PhD<br>
    EBOSS Research Day 2018<br>
    

    
</td>
</tr>





</table>
